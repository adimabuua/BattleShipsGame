{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3Qjh8mRHqD"
      },
      "source": [
        "# Assignment 2: N-grams and Language Identification\n",
        "## CNG463 - Introduction to Natural Language Processing\n",
        "### METU NCC Computer Engineering | Fall 2025-26\n",
        "\n",
        "**Student Name:** Emmanuel Monye  \n",
        "**Student ID:** 2406254\n",
        "\n",
        "**Due Date:** 16 November 2025 (Sunday) before midnight\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDEnwcxRHqE"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This assignment focuses on:\n",
        "1. Building **character-based** 2-gram and 3-gram language models with Laplace smoothing\n",
        "2. Sentence-based language identification using 10-fold cross-validation\n",
        "3. Evaluation using accuracy, precision, recall, and F1-score\n",
        "4. Comparison and analysis\n",
        "\n",
        "**Note:** For language identification, we use **character n-grams** rather than word n-grams because they better capture language-specific patterns like letter combinations, diacritics, and writing systems.\n",
        "\n",
        "**Grading:**\n",
        "- Written Questions (7 Ã— 4 pts): **28 pts**\n",
        "- Code Tasks with TODO (11 total): **72 pts** distributed by effort level:\n",
        "  - Simple tasks: 4 pts each (2 cells)\n",
        "  - Moderate tasks: 6 pts each (4 cells)\n",
        "  - Complex tasks: 8 pts each (5 cells)\n",
        "- **Total: 100 pts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srF42PZXRHqP"
      },
      "source": [
        "---\n",
        "\n",
        "## Pre-Submission Checklist\n",
        "\n",
        "- [ ] Name and student ID at top\n",
        "- [ ] No cells are added or removed\n",
        "- [ ] All TODO sections completed\n",
        "- [ ] All questions answered\n",
        "- [ ] Code runs without errors\n",
        "- [ ] Results tables included\n",
        "- [ ] Run All before saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0Xw3YQRHqE"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "gwyLNfKaRHqE"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Tuple, Dict\n",
        "import re\n",
        "\n",
        "# Scikit-learn for cross-validation and metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr-UNnAGRHqF"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: Corpus Preparation and Statistics (22 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5k13c4MRHqF"
      },
      "source": [
        "## 1.1: Upload Corpus Files\n",
        "\n",
        "Prepare your text files in **two different languages** (accepted formats: `.txt`, `.pdf`, or `.docx`). When you run the cell below, you'll be prompted to upload files for each language separately. Make sure your files contain substantial text (reports, essays, or similar content from other courses). Each language requires at least **5000** words in its corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "cb6tjNLmRHqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "182f4dc8-ec4e-4623-8857-b9bfe54814d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your ENGLISH corpus file(s):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0bba52a8-b66c-46c7-8d0b-6d3f0845c97e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0bba52a8-b66c-46c7-8d0b-6d3f0845c97e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving english.txt to english (3).txt\n",
            "Saving english2.txt to english2 (2).txt\n",
            "\n",
            "Upload your SECOND LANGUAGE corpus file(s):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba459338-1510-47af-92e3-1e88183c55cf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ba459338-1510-47af-92e3-1e88183c55cf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving french.txt to french (2).txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your ENGLISH corpus file(s):\")\n",
        "english_files = files.upload()\n",
        "\n",
        "print(\"\\nUpload your SECOND LANGUAGE corpus file(s):\")\n",
        "second_lang_files = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDDP9gORHqF"
      },
      "source": [
        "## 1.2: Load and Preprocess Data (12 points)\n",
        "\n",
        "Load your uploaded files, extract text, preprocess, split into sentences, and tokenize. You'll need helper functions to handle different file formats.\n",
        "\n",
        "**Steps:**\n",
        "1. Read files based on format (`.txt`, `.pdf`, `.docx`) and combine them into single text for each language\n",
        "2. Apply preprocessing (e.g., lowercasing, handling punctuation)\n",
        "3. Split each corpus into individual sentences\n",
        "4. Tokenize each sentence into words (for statistics)\n",
        "5. Store the results as two lists of tokenized sentences\n",
        "\n",
        "**Important:** You'll use word tokenization for calculating statistics, but for the n-gram models in Task 2, you'll work with character n-grams directly on the sentence strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "FI_9lAXTRHqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74404700-abd1-4434-b3b4-a4dd8efaaafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing English files...\n",
            "Processed english (3).txt successfully\n",
            "Processed english2 (2).txt successfully\n",
            "English: 687 sentences, 17404 words\n",
            "\n",
            "Processing Second Language files...\n",
            "Processed french (2).txt successfully\n",
            "Second Language: 212 sentences, 4280 words\n",
            "\n",
            "Final Counts:\n",
            "English: 687 sentences\n",
            "Second Language: 212 sentences\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import List\n",
        "import PyPDF2\n",
        "import docx\n",
        "\n",
        "def read_txt_file(filename: str) -> str:\n",
        "    \"\"\"Read a .txt file and return its content.\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def read_pdf_file(filename: str) -> str:\n",
        "    \"\"\"Read a .pdf file and return its text content.\"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        text = ''\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    # TODO: Install and use PyPDF2 or pdfplumber\n",
        "    # Example: pip install PyPDF2\n",
        "    pass\n",
        "\n",
        "def read_docx_file(filename: str) -> str:\n",
        "    \"\"\"Read a .docx file and return its text content.\"\"\"\n",
        "    doc = docx.Document(filename)\n",
        "    text = ''\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + '\\n'\n",
        "    return text\n",
        "    # TODO: Install and use python-docx\n",
        "    # Example: pip install python-docx\n",
        "    pass\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    \"\"\"Split text into sentences.\"\"\"\n",
        "    # Split on .!? followed by whitespace or end of string\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    # Remove empty sentences and strip whitespace\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "    # TODO: Implement sentence splitting\n",
        "    # You can use simple regex or nltk.sent_tokenize\n",
        "    pass\n",
        "\n",
        "def tokenize_sentence(sentence: str) -> List[str]:\n",
        "    \"\"\"Tokenize a sentence into words.\"\"\"\n",
        "    words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "    return words\n",
        "\n",
        "    # TODO: Implement word tokenization\n",
        "    # You can use str.split() or nltk.word_tokenize\n",
        "    pass\n",
        "\n",
        "def process_files(file_dict, language_name):\n",
        "    \"\"\"Process uploaded files for a language.\"\"\"\n",
        "    all_text = \"\"\n",
        "    for filename in file_dict.keys():\n",
        "        try:\n",
        "            if filename.lower().endswith('.txt'):\n",
        "                text = read_txt_file(filename)\n",
        "            elif filename.lower().endswith('.pdf'):\n",
        "                text = read_pdf_file(filename)\n",
        "            elif filename.lower().endswith('.docx'):\n",
        "                text = read_docx_file(filename)\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {filename}\")\n",
        "                continue\n",
        "\n",
        "            all_text += text + \" \"\n",
        "            print(f\"Processed {filename} successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # Preprocess: lowercase and remove extra whitespace\n",
        "    all_text = all_text.lower()\n",
        "    all_text = re.sub(r'\\s+', ' ', all_text).strip()\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = split_into_sentences(all_text)\n",
        "\n",
        "    # Tokenize sentences\n",
        "    tokenized_sentences = [tokenize_sentence(sent) for sent in sentences]\n",
        "\n",
        "    print(f\"{language_name}: {len(sentences)} sentences, {sum(len(s) for s in tokenized_sentences)} words\")\n",
        "    return sentences, tokenized_sentences\n",
        "\n",
        "# Process both languages\n",
        "print(\"Processing English files...\")\n",
        "lang1_sentences, lang1_sentences_tokenized = process_files(english_files, \"English\")\n",
        "\n",
        "print(\"\\nProcessing Second Language files...\")\n",
        "lang2_sentences, lang2_sentences_tokenized = process_files(second_lang_files, \"Second Language\")\n",
        "\n",
        "print(f\"\\nFinal Counts:\")\n",
        "print(f\"English: {len(lang1_sentences)} sentences\")\n",
        "print(f\"Second Language: {len(lang2_sentences)} sentences\")\n",
        "\n",
        "# TODO:\n",
        "#\n",
        "# 1. Read and combine files for each language\n",
        "#    - Loop through lang1_files and lang2_files\n",
        "#    - Use appropriate read function based on file extension\n",
        "#    - Combine all text into lang1_text and lang2_text\n",
        "#\n",
        "# 2. Apply preprocessing to both lang1_text and lang2_text\n",
        "#    (e.g., lowercasing, removing extra whitespace)\n",
        "#\n",
        "# 3. Split each corpus into sentences using split_into_sentences()\n",
        "#\n",
        "# 4. Tokenize each sentence using tokenize_sentence()\n",
        "#\n",
        "# Note: These tokenized sentences will be used for statistics in Task 1.\n",
        "# In Task 2, you'll work with the raw sentence strings for character n-grams.\n",
        "\n",
        "# At the end, you should have:\n",
        "# lang1_sentences = [sent1, sent2, ...]\n",
        "# lang2_sentences = [sent1, sent2, ...]\n",
        "# lang1_sentences_tokenized = [[word1, word2, ...], [word1, word2, ...], ...]\n",
        "# lang2_sentences_tokenized = [[word1, word2, ...], [word1, word2, ...], ...]\n",
        "\n",
        "\n",
        "# [8 pts]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wmeTEZ-WkEm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg18ezdTRHqG"
      },
      "source": [
        "**Question 1.1:** What preprocessing choices did you make and why? (3-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLjQTbejRHqG"
      },
      "source": [
        "I chose to lowercase all text to ensure case-insensitive processing, which is common in language identification tasks. For sentence splitting, I used regex-based splitting on punctuation marks followed by whitespace, which works reasonably well for most European languages (English as my first language and French as my second). For tokenization, I used word boundary regex to extract words while removing punctuation. These choices help normalize the text and focus on linguistic patterns rather than formatting variations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prIqhFYyRHqG"
      },
      "source": [
        "## 1.3: Basic Statistics (10 points)\n",
        "\n",
        "Calculate and display key statistics for both language corpora to understand their characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yvSoUoATRHqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79a7b3e-fa4c-4f2d-f728-d3384a264260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English Statistics:\n",
            "  Total characters: 97078\n",
            "  Special characters: 2227\n",
            "  Character vocabulary: 57\n",
            "  Total words: 17404\n",
            "  Word vocabulary: 3538\n",
            "  Sentence count: 687\n",
            "  Average sentence length: 25.33 words\n",
            "\n",
            "Second Language Statistics:\n",
            "  Total characters: 26549\n",
            "  Special characters: 931\n",
            "  Character vocabulary: 65\n",
            "  Total words: 4280\n",
            "  Word vocabulary: 1495\n",
            "  Sentence count: 212\n",
            "  Average sentence length: 20.19 words\n"
          ]
        }
      ],
      "source": [
        "def calculate_corpus_statistics(sentences, tokenized_sentences, language_name):\n",
        "    \"\"\"Calculate and display statistics for a corpus.\"\"\"\n",
        "    # Character statistics\n",
        "    total_chars = sum(len(sentence) for sentence in sentences)\n",
        "    special_chars = sum(len(re.findall(r'[^\\w\\s]', sentence)) for sentence in sentences)\n",
        "    char_vocab = len(set(''.join(sentences)))\n",
        "\n",
        "    # Word statistics\n",
        "    all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
        "    total_words = len(all_words)\n",
        "    word_vocab = len(set(all_words))\n",
        "\n",
        "    # Sentence statistics\n",
        "    sentence_count = len(sentences)\n",
        "    avg_sentence_length = total_words / sentence_count if sentence_count > 0 else 0\n",
        "\n",
        "    print(f\"\\n{language_name} Statistics:\")\n",
        "    print(f\"  Total characters: {total_chars}\")\n",
        "    print(f\"  Special characters: {special_chars}\")\n",
        "    print(f\"  Character vocabulary: {char_vocab}\")\n",
        "    print(f\"  Total words: {total_words}\")\n",
        "    print(f\"  Word vocabulary: {word_vocab}\")\n",
        "    print(f\"  Sentence count: {sentence_count}\")\n",
        "    print(f\"  Average sentence length: {avg_sentence_length:.2f} words\")\n",
        "\n",
        "    return {\n",
        "        'total_chars': total_chars,\n",
        "        'special_chars': special_chars,\n",
        "        'char_vocab': char_vocab,\n",
        "        'total_words': total_words,\n",
        "        'word_vocab': word_vocab,\n",
        "        'sentence_count': sentence_count,\n",
        "        'avg_sentence_length': avg_sentence_length\n",
        "    }\n",
        "\n",
        "# Calculate statistics for both languages\n",
        "stats_lang1 = calculate_corpus_statistics(lang1_sentences, lang1_sentences_tokenized, \"English\")\n",
        "stats_lang2 = calculate_corpus_statistics(lang2_sentences, lang2_sentences_tokenized, \"Second Language\")\n",
        "\n",
        "# TODO: Calculate statistics for BOTH languages\n",
        "#\n",
        "# For each language (lang1_sentences and lang2_sentences):\n",
        "# - Total character count\n",
        "# - Special character/punctuation count\n",
        "# - Character vocabulary size (unique characters)\n",
        "# - Total word count\n",
        "# - Word vocabulary size (unique words)\n",
        "# - Sentence count\n",
        "# - Average sentence length (in words)\n",
        "#\n",
        "# Example structure:\n",
        "# lang1_total_characters = sum(len(sentence) for sentence in lang1_sentences)\n",
        "# ...\n",
        "# lang1_total_words = sum(len(sentence) for sentence in lang1_sentences_tokenized)\n",
        "# lang1_word_vocabulary = len(set(word for sentence in lang1_sentences_tokenized for word in sentence))\n",
        "# ...\n",
        "#\n",
        "# Print statistics side by side for the two corpora\n",
        "\n",
        "\n",
        "# [6 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-PSkjPGRHqG"
      },
      "source": [
        "**Question 1.2:** What are the key differences between your two corpora? (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS9Y_nWwRHqG"
      },
      "source": [
        "The key differences between my corpora include vocabulary size, average sentence length, and character distribution. English typically has shorter average word length compared to many other languages. French has different character sets or special characters that affect the character vocabulary size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMnhFykGRHqG"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Character N-gram Language Identification (58 points)\n",
        "\n",
        "**Baseline (46 pts):** Implement character-based 2-gram and 3-gram models, run 10-fold CV, report accuracy.  \n",
        "**Creativity (12 pts):** Out-of-vocabulary analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugQ7EB70RHqG"
      },
      "source": [
        "## 2.1: Implement Character N-gram Models (12 points)\n",
        "\n",
        "Implement the `CharNgramLanguageModel` class with Laplace smoothing using NLTK's n-gram utilities. The model should count **character** n-grams during training and calculate sentence probabilities with smoothing.\n",
        "\n",
        "**Key difference from word n-grams:** Instead of tokenizing sentences into words, you'll work with individual characters in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "tKiKoqstRHqG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams, pad_sequence\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from typing import List\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "class CharNgramLanguageModel:\n",
        "    \"\"\"\n",
        "    Character-based N-gram language model with Laplace (add-1) smoothing using NLTK.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int = 2):\n",
        "        \"\"\"\n",
        "        Initialize the character n-gram model.\n",
        "\n",
        "        Args:\n",
        "            n: Order of n-gram (2 for bigram, 3 for trigram)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.model = Laplace(n)\n",
        "\n",
        "    def train(self, sentences: List[str]):\n",
        "        \"\"\"\n",
        "        Train the model on a list of sentences.\n",
        "\n",
        "        Args:\n",
        "            sentences: List of sentences (each sentence is a string)\n",
        "        \"\"\"\n",
        "        # TODO: Your code here\n",
        "        # Convert each sentence to a list of characters\n",
        "        # Example: \"hello\" -> ['h', 'e', 'l', 'l', 'o']\n",
        "        # Use padded_everygram_pipeline to prepare training data with padding\n",
        "        # Then fit the model using self.model.fit()\n",
        "\n",
        "        char_sequences = []\n",
        "        for sentence in sentences:\n",
        "            chars = list(sentence)\n",
        "            char_sequences.append(chars)\n",
        "\n",
        "        train_data, padded_vocab = padded_everygram_pipeline(self.n, char_sequences)\n",
        "\n",
        "        # Fit the model\n",
        "        self.model.fit(train_data, padded_vocab)\n",
        "        pass\n",
        "\n",
        "    def get_probability(self, sentence: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the probability of a sentence.\n",
        "\n",
        "        Args:\n",
        "            sentence: Sentence string\n",
        "\n",
        "        Returns:\n",
        "            Probability of the sentence\n",
        "        \"\"\"\n",
        "        # TODO: Your code here\n",
        "        # Convert sentence to list of characters\n",
        "        # Pad the character sequence and generate n-grams\n",
        "        # For each n-gram, get probability using self.model.score()\n",
        "        # Multiply probabilities together (or sum log probabilities to avoid underflow)\n",
        "\n",
        "        chars = list(sentence)\n",
        "\n",
        "        # Generate n-grams with padding\n",
        "        ngrams_list = list(ngrams(pad_sequence(chars, pad_left=True, pad_right=True,\n",
        "                                              left_pad_symbol='<s>', right_pad_symbol='</s>',\n",
        "                                              n=self.n), self.n))\n",
        "\n",
        "        # Calculate log probability to avoid underflow\n",
        "        log_prob = 0.0\n",
        "        for ngram in ngrams_list:\n",
        "            prob = self.model.score(ngram[-1], ngram[:-1])\n",
        "            log_prob += np.log(prob) if prob > 0 else -np.inf\n",
        "\n",
        "        return np.exp(log_prob) if log_prob > -np.inf else 0.0\n",
        "        pass\n",
        "\n",
        "# [8 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwxpPMNwRHqG"
      },
      "source": [
        "### Spot Check: Inspect Your N-gram Models\n",
        "\n",
        "After implementing the model, train sample models on both languages and inspect what they learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "TT9Q6WH8RHqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b306050-cda9-4935-e021-65ea3ffd69c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English 2-gram vocabulary size: 60\n",
            "English 3-gram vocabulary size: 60\n",
            "French 2-gram vocabulary size: 68\n",
            "French 3-gram vocabulary size: 68\n",
            "\n",
            "Sample sentence: 'This is a test sentence.'\n",
            "English 2-gram prob: 6.03e-27\n",
            "French 2-gram prob: 2.91e-26\n"
          ]
        }
      ],
      "source": [
        "# TODO: Train sample models and inspect them\n",
        "#\n",
        "# 1. Create 2-gram and 3-gram models for both languages\n",
        "# 2. Train them on your full datasets (lang1_sentences and lang2_sentences)\n",
        "# 3. Inspect the models to see what n-grams they learned\n",
        "#\n",
        "# Example:\n",
        "# model_2gram_lang1 = NgramLanguageModel(n=2)\n",
        "# model_2gram_lang1.train(lang1_sentences)\n",
        "# model_3gram_lang1 = NgramLanguageModel(n=3)\n",
        "# model_3gram_lang1.train(lang1_sentences)\n",
        "#\n",
        "# model_2gram_lang2 = NgramLanguageModel(n=2)\n",
        "# model_2gram_lang2.train(lang2_sentences)\n",
        "# model_3gram_lang2 = NgramLanguageModel(n=3)\n",
        "# model_3gram_lang2.train(lang3_sentences)\n",
        "#\n",
        "# Display sample n-grams and their counts from each model\n",
        "# Check vocabulary size: len(model.model.vocab)\n",
        "# Show most common n-grams or test probabilities on sample sentences\n",
        "\n",
        "# Your code here\n",
        "\n",
        "model_2gram_lang1 = CharNgramLanguageModel(n=2)\n",
        "model_2gram_lang1.train(lang1_sentences)\n",
        "model_3gram_lang1 = CharNgramLanguageModel(n=3)\n",
        "model_3gram_lang1.train(lang1_sentences)\n",
        "\n",
        "model_2gram_lang2 = CharNgramLanguageModel(n=2)\n",
        "model_2gram_lang2.train(lang2_sentences)\n",
        "model_3gram_lang2 = CharNgramLanguageModel(n=3)\n",
        "model_3gram_lang2.train(lang2_sentences)\n",
        "\n",
        "# Inspect models\n",
        "print(f\"English 2-gram vocabulary size: {len(model_2gram_lang1.model.vocab)}\")\n",
        "print(f\"English 3-gram vocabulary size: {len(model_3gram_lang1.model.vocab)}\")\n",
        "print(f\"French 2-gram vocabulary size: {len(model_2gram_lang2.model.vocab)}\")\n",
        "print(f\"French 3-gram vocabulary size: {len(model_3gram_lang2.model.vocab)}\")\n",
        "\n",
        "# Test on sample sentences\n",
        "test_sentence = \"This is a test sentence.\"\n",
        "print(f\"\\nSample sentence: '{test_sentence}'\")\n",
        "print(f\"English 2-gram prob: {model_2gram_lang1.get_probability(test_sentence):.2e}\")\n",
        "print(f\"French 2-gram prob: {model_2gram_lang2.get_probability(test_sentence):.2e}\")\n",
        "\n",
        "# [4 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olw1RKdrRHqG"
      },
      "source": [
        "## 2.2: Implement Language Identification (8 points)\n",
        "\n",
        "Create a function that compares sentence probabilities from two language models and returns the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ygVJnY5gRHqG"
      },
      "outputs": [],
      "source": [
        "def identify_language(sentence: str,\n",
        "                     model_lang1: CharNgramLanguageModel,\n",
        "                     model_lang2: CharNgramLanguageModel) -> int:\n",
        "    \"\"\"\n",
        "    Identify the language of a sentence using two character-based language models.\n",
        "\n",
        "    Args:\n",
        "        sentence: Sentence string\n",
        "        model_lang1: Language model for language 1 (label 0)\n",
        "        model_lang2: Language model for language 2 (label 1)\n",
        "\n",
        "    Returns:\n",
        "        Predicted label (0 or 1)\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Calculate probability of sentence using model_lang1\n",
        "    #    prob1 = model_lang1.get_probability(sentence)\n",
        "    #\n",
        "    # 2. Calculate probability of sentence using model_lang2\n",
        "    #    prob2 = model_lang2.get_probability(sentence)\n",
        "    #\n",
        "    # 3. Compare probabilities and return the label of the model with higher probability\n",
        "    #    if prob1 > prob2: return 0\n",
        "    #    else: return 1\n",
        "\n",
        "    prob1 = model_lang1.get_probability(sentence)\n",
        "    prob2 = model_lang2.get_probability(sentence)\n",
        "\n",
        "    if prob1 > prob2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "    pass\n",
        "\n",
        "# [8 pts]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v9cJXjQRHqG"
      },
      "source": [
        "## 2.3: Implement Evaluation Function (6 points)\n",
        "\n",
        "Create a function that calculates accuracy, precision, recall, and F1-score given predicted and true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "QsO4Rwz6RHqH"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with accuracy, precision, recall, f1_score\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    # Use sklearn's accuracy_score and precision_recall_fscore_support\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "    pass\n",
        "\n",
        "# [6 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpUfSf2RHqH"
      },
      "source": [
        "## 2.4: 10-Fold Cross-Validation for Language Identification (8 points)\n",
        "\n",
        "Implement 10-fold cross-validation to evaluate your character-based n-gram models. In each fold, split the data, train separate models for each language and n-gram order, make predictions, and evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "sqLvr7twRHqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9807b049-1a9a-4ca9-80f5-3b82fcb821f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset prepared:\n",
            "  Total sentences: 899\n",
            "  Language 1 (label 0): 687 sentences\n",
            "  Language 2 (label 1): 212 sentences\n",
            "\n",
            "\n",
            "==================================================\n",
            "Fold 1/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.956\n",
            "3-gram - Accuracy: 0.978\n",
            "\n",
            "==================================================\n",
            "Fold 2/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.956\n",
            "3-gram - Accuracy: 0.989\n",
            "\n",
            "==================================================\n",
            "Fold 3/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.967\n",
            "3-gram - Accuracy: 0.989\n",
            "\n",
            "==================================================\n",
            "Fold 4/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.944\n",
            "3-gram - Accuracy: 0.956\n",
            "\n",
            "==================================================\n",
            "Fold 5/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.911\n",
            "3-gram - Accuracy: 0.956\n",
            "\n",
            "==================================================\n",
            "Fold 6/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.967\n",
            "3-gram - Accuracy: 0.978\n",
            "\n",
            "==================================================\n",
            "Fold 7/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.944\n",
            "3-gram - Accuracy: 0.978\n",
            "\n",
            "==================================================\n",
            "Fold 8/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.944\n",
            "3-gram - Accuracy: 0.967\n",
            "\n",
            "==================================================\n",
            "Fold 9/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.944\n",
            "3-gram - Accuracy: 0.978\n",
            "\n",
            "==================================================\n",
            "Fold 10/10\n",
            "==================================================\n",
            "2-gram - Accuracy: 0.955\n",
            "3-gram - Accuracy: 0.978\n",
            "\n",
            "==================================================\n",
            "Cross-validation completed!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Prepare dataset: combine sentence STRINGS from both languages with labels\n",
        "X = lang1_sentences + lang2_sentences\n",
        "y = [0] * len(lang1_sentences) + [1] * len(lang2_sentences)\n",
        "\n",
        "print(f\"Dataset prepared:\")\n",
        "print(f\"  Total sentences: {len(X)}\")\n",
        "print(f\"  Language 1 (label 0): {sum(1 for label in y if label == 0)} sentences\")\n",
        "print(f\"  Language 2 (label 1): {sum(1 for label in y if label == 1)} sentences\")\n",
        "print()\n",
        "\n",
        "# Initialize 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results for each fold\n",
        "results_2gram = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "results_3gram = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "# TODO: Implement 10-fold cross-validation\n",
        "#\n",
        "# For each fold, you need to:\n",
        "#   1. Split data into train and test sets using train_idx and test_idx from kfold.split(X)\n",
        "#   2. Separate training sentences by language based on labels\n",
        "#   3. Train FOUR models total:\n",
        "#      - Two 2-gram models (one per language)\n",
        "#      - Two 3-gram models (one per language)\n",
        "#   4. Make predictions on test sentences using identify_language()\n",
        "#   5. Calculate metrics using calculate_metrics()\n",
        "#   6. Store results in results_2gram and results_3gram dictionaries\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X), 1):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Fold {fold_idx}/10\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # TODO Your implementation here\n",
        "\n",
        "    X_train = [X[i] for i in train_idx]\n",
        "    y_train = [y[i] for i in train_idx]\n",
        "    X_test = [X[i] for i in test_idx]\n",
        "    y_test = [y[i] for i in test_idx]\n",
        "\n",
        "    # Separate training sentences by language\n",
        "    train_lang1 = [X_train[i] for i in range(len(X_train)) if y_train[i] == 0]\n",
        "    train_lang2 = [X_train[i] for i in range(len(X_train)) if y_train[i] == 1]\n",
        "\n",
        "    # Train models\n",
        "    # 2-gram models\n",
        "    model_2gram_lang1 = CharNgramLanguageModel(n=2)\n",
        "    model_2gram_lang1.train(train_lang1)\n",
        "    model_2gram_lang2 = CharNgramLanguageModel(n=2)\n",
        "    model_2gram_lang2.train(train_lang2)\n",
        "\n",
        "    # 3-gram models\n",
        "    model_3gram_lang1 = CharNgramLanguageModel(n=3)\n",
        "    model_3gram_lang1.train(train_lang1)\n",
        "    model_3gram_lang2 = CharNgramLanguageModel(n=3)\n",
        "    model_3gram_lang2.train(train_lang2)\n",
        "\n",
        "    # Make predictions\n",
        "    pred_2gram = [identify_language(sent, model_2gram_lang1, model_2gram_lang2) for sent in X_test]\n",
        "    pred_3gram = [identify_language(sent, model_3gram_lang1, model_3gram_lang2) for sent in X_test]\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_2gram = calculate_metrics(y_test, pred_2gram)\n",
        "    metrics_3gram = calculate_metrics(y_test, pred_3gram)\n",
        "\n",
        "    # Store results\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "        results_2gram[metric].append(metrics_2gram[metric])\n",
        "        results_3gram[metric].append(metrics_3gram[metric])\n",
        "\n",
        "    print(f\"2-gram - Accuracy: {metrics_2gram['accuracy']:.3f}\")\n",
        "    print(f\"3-gram - Accuracy: {metrics_3gram['accuracy']:.3f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Cross-validation completed!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# [8 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlXE_SfFRHqH"
      },
      "source": [
        "## 2.5: Display Results (12)\n",
        "\n",
        "*Create a table showing for each model:*\n",
        "Mean accuracy, precision, recall, F1 (with std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "cyBVJGxFRHqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0e064d-8e02-4dd0-c702-a8d04ffd031a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Results (Mean Â± Standard Deviation):\n",
            " Model      Accuracy     Precision        Recall      F1-Score\n",
            "2-gram 0.949 Â± 0.015 0.831 Â± 0.052 0.978 Â± 0.027 0.898 Â± 0.035\n",
            "3-gram 0.974 Â± 0.011 0.903 Â± 0.051 0.996 Â± 0.013 0.946 Â± 0.027\n"
          ]
        }
      ],
      "source": [
        "# TODO: Calculate and display summary statistics\n",
        "#\n",
        "#\n",
        "# Example:\n",
        "# results_df = pd.DataFrame({\n",
        "#     'Model': ['2-gram', '3-gram'],\n",
        "#     'Accuracy': [...],\n",
        "#     'Precision': [...],\n",
        "#     ...\n",
        "# })\n",
        "\n",
        "def calculate_summary(results):\n",
        "    return {\n",
        "        'mean_accuracy': np.mean(results['accuracy']),\n",
        "        'std_accuracy': np.std(results['accuracy']),\n",
        "        'mean_precision': np.mean(results['precision']),\n",
        "        'std_precision': np.std(results['precision']),\n",
        "        'mean_recall': np.mean(results['recall']),\n",
        "        'std_recall': np.std(results['recall']),\n",
        "        'mean_f1': np.mean(results['f1']),\n",
        "        'std_f1': np.std(results['f1'])\n",
        "    }\n",
        "\n",
        "summary_2gram = calculate_summary(results_2gram)\n",
        "summary_3gram = calculate_summary(results_3gram)\n",
        "\n",
        "# Create results table\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['2-gram', '3-gram'],\n",
        "    'Accuracy': [f\"{summary_2gram['mean_accuracy']:.3f} Â± {summary_2gram['std_accuracy']:.3f}\",\n",
        "                f\"{summary_3gram['mean_accuracy']:.3f} Â± {summary_3gram['std_accuracy']:.3f}\"],\n",
        "    'Precision': [f\"{summary_2gram['mean_precision']:.3f} Â± {summary_2gram['std_precision']:.3f}\",\n",
        "                 f\"{summary_3gram['mean_precision']:.3f} Â± {summary_3gram['std_precision']:.3f}\"],\n",
        "    'Recall': [f\"{summary_2gram['mean_recall']:.3f} Â± {summary_2gram['std_recall']:.3f}\",\n",
        "              f\"{summary_3gram['mean_recall']:.3f} Â± {summary_3gram['std_recall']:.3f}\"],\n",
        "    'F1-Score': [f\"{summary_2gram['mean_f1']:.3f} Â± {summary_2gram['std_f1']:.3f}\",\n",
        "                f\"{summary_3gram['mean_f1']:.3f} Â± {summary_3gram['std_f1']:.3f}\"]\n",
        "})\n",
        "\n",
        "print(\"Cross-Validation Results (Mean Â± Standard Deviation):\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# [4 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7XedIvGRHqH"
      },
      "source": [
        "**Question 2.1:** Which of your trained models performed best on the validation data, and why? (3-4 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz1s_WlARHqH"
      },
      "source": [
        "The 3 gram model performed best. The 3-gram model typically performs better because it captures more contextual information about character sequences. However, if the training data is limited, the 2-gram model might perform better due to having more reliable probability estimates with less data sparsity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckv543M4RHqH"
      },
      "source": [
        "**Question 2.2:** Were the results consistent across different folds of cross-validation? (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkbHF5HbRHqH"
      },
      "source": [
        "Results were generally consistent across folds, but some variation occurred due to different data splits. Folds with more balanced or representative samples showed higher performance, while folds with unusual sentence patterns showed slightly lower metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk7uadeZRHqH"
      },
      "source": [
        "## 2.6: Out-of-Vocabulary Testing (12 pts)\n",
        "\n",
        "Test your models with **five** sentences containing characters or character combinations not common in your training corpus. For character n-grams, this might include unusual letter combinations, foreign words, or made-up words that still follow language patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "dj0QEDnARHqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe775fee-a6ca-4df4-bc38-5050d835d5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Vocabulary Testing:\n",
            "==================================================\n",
            "\n",
            "OOV Sentence 1: 'The quantum flux capacitor oscillates at 1.21 gigawatts'\n",
            "2-gram: Lang1 prob=3.37e-79, Lang2 prob=1.43e-77, Predicted: 1\n",
            "3-gram: Lang1 prob=1.85e-86, Lang2 prob=9.44e-90, Predicted: 0\n",
            "\n",
            "OOV Sentence 2: 'SchrÃ¶dinger's cat is both alive and dead simultaneously'\n",
            "2-gram: Lang1 prob=1.03e-71, Lang2 prob=2.38e-82, Predicted: 0\n",
            "3-gram: Lang1 prob=1.68e-74, Lang2 prob=2.75e-90, Predicted: 0\n",
            "\n",
            "OOV Sentence 3: 'Pneumonoultramicroscopicsilicovolcanoconiosis is a lung disease'\n",
            "2-gram: Lang1 prob=8.56e-86, Lang2 prob=1.48e-88, Predicted: 0\n",
            "3-gram: Lang1 prob=1.07e-96, Lang2 prob=7.30e-110, Predicted: 0\n",
            "\n",
            "OOV Sentence 4: 'I ate crÃªpes with jalapeÃ±os at the cafÃ©'\n",
            "2-gram: Lang1 prob=5.50e-55, Lang2 prob=7.17e-58, Predicted: 0\n",
            "3-gram: Lang1 prob=2.92e-57, Lang2 prob=5.11e-69, Predicted: 0\n",
            "\n",
            "OOV Sentence 5: 'The zephyr blew through the xylem of the baobab tree'\n",
            "2-gram: Lang1 prob=7.79e-78, Lang2 prob=4.76e-92, Predicted: 0\n",
            "3-gram: Lang1 prob=4.98e-72, Lang2 prob=2.47e-95, Predicted: 0\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create and test OOV sentences\n",
        "#\n",
        "# - Be creative!\n",
        "\n",
        "# Create OOV sentences\n",
        "oov_sentences = [\n",
        "    \"The quantum flux capacitor oscillates at 1.21 gigawatts\",  # Technical terms\n",
        "    \"SchrÃ¶dinger's cat is both alive and dead simultaneously\",  # Special characters\n",
        "    \"Pneumonoultramicroscopicsilicovolcanoconiosis is a lung disease\",  # Very long word\n",
        "    \"I ate crÃªpes with jalapeÃ±os at the cafÃ©\",  # Mixed diacritics\n",
        "    \"The zephyr blew through the xylem of the baobab tree\"  # Uncommon words\n",
        "]\n",
        "\n",
        "print(\"Out-of-Vocabulary Testing:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Train models on full data for OOV testing\n",
        "model_2gram_lang1_full = CharNgramLanguageModel(n=2)\n",
        "model_2gram_lang1_full.train(lang1_sentences)\n",
        "model_2gram_lang2_full = CharNgramLanguageModel(n=2)\n",
        "model_2gram_lang2_full.train(lang2_sentences)\n",
        "\n",
        "model_3gram_lang1_full = CharNgramLanguageModel(n=3)\n",
        "model_3gram_lang1_full.train(lang1_sentences)\n",
        "model_3gram_lang2_full = CharNgramLanguageModel(n=3)\n",
        "model_3gram_lang2_full.train(lang2_sentences)\n",
        "\n",
        "for i, sentence in enumerate(oov_sentences, 1):\n",
        "    pred_2gram = identify_language(sentence, model_2gram_lang1_full, model_2gram_lang2_full)\n",
        "    pred_3gram = identify_language(sentence, model_3gram_lang1_full, model_3gram_lang2_full)\n",
        "\n",
        "    prob1_2gram = model_2gram_lang1_full.get_probability(sentence)\n",
        "    prob2_2gram = model_2gram_lang2_full.get_probability(sentence)\n",
        "    prob1_3gram = model_3gram_lang1_full.get_probability(sentence)\n",
        "    prob2_3gram = model_3gram_lang2_full.get_probability(sentence)\n",
        "\n",
        "    print(f\"\\nOOV Sentence {i}: '{sentence}'\")\n",
        "    print(f\"2-gram: Lang1 prob={prob1_2gram:.2e}, Lang2 prob={prob2_2gram:.2e}, Predicted: {pred_2gram}\")\n",
        "    print(f\"3-gram: Lang1 prob={prob1_3gram:.2e}, Lang2 prob={prob2_3gram:.2e}, Predicted: {pred_3gram}\")\n",
        "\n",
        "# [8 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqrkgUOARHqH"
      },
      "source": [
        "**Question 2.3:** How well did your models handle out-of-vocabulary (OOV) samples? (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taFkBF3JRHqH"
      },
      "source": [
        "The models handled OOV samples reasonably well by relying on character-level patterns rather than complete word recognition. Sentences with character combinations typical of the training language were correctly identified, while those with unusual patterns showed lower confidence scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz19fjohRHqH"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 3: Statistical Analysis (20 points)\n",
        "\n",
        "**Baseline (10 pts):** Statistical significance testing and comparison.  \n",
        "**Creativity (10 pts):** Advanced analysis (confusion matrices, error analysis, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peCDCXqqRHqO"
      },
      "source": [
        "## 3.1: Statistical Significance Testing (10 points)\n",
        "\n",
        "Use paired t-test to compare models. p-value < 0.05 indicates statistically significant difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "nNrEt4s5RHqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1097e2eb-cdba-4571-a5d9-fc6cb6728f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical Significance Testing:\n",
            "========================================\n",
            "Accuracy comparison - t-statistic: -7.680, p-value: 0.000\n",
            "F1-score comparison - t-statistic: -8.354, p-value: 0.000\n",
            "\n",
            "The difference in accuracy between 2-gram and 3-gram models is statistically significant (p < 0.05)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Perform paired t-tests\n",
        "#\n",
        "# Compare: 2-gram vs 3-gram\n",
        "#\n",
        "# Use: t_stat, p_value = ttest_rel(results_1, results_2)\n",
        "\n",
        "# Perform paired t-tests\n",
        "t_stat_accuracy, p_value_accuracy = ttest_rel(results_2gram['accuracy'], results_3gram['accuracy'])\n",
        "t_stat_f1, p_value_f1 = ttest_rel(results_2gram['f1'], results_3gram['f1'])\n",
        "\n",
        "print(\"Statistical Significance Testing:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy comparison - t-statistic: {t_stat_accuracy:.3f}, p-value: {p_value_accuracy:.3f}\")\n",
        "print(f\"F1-score comparison - t-statistic: {t_stat_f1:.3f}, p-value: {p_value_f1:.3f}\")\n",
        "\n",
        "if p_value_accuracy < 0.05:\n",
        "    print(\"\\nThe difference in accuracy between 2-gram and 3-gram models is statistically significant (p < 0.05)\")\n",
        "else:\n",
        "    print(\"\\nThe difference in accuracy between 2-gram and 3-gram models is NOT statistically significant (p â‰¥ 0.05)\")\n",
        "\n",
        "# [6 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20soxRd6RHqO"
      },
      "source": [
        "**Question 3.1:** Are the performance differences statistically significant? Explain what 'statistical significance' means in this context. (2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48KiLvz4RHqP"
      },
      "source": [
        "The difference in accuracy are statistically significant. Statistical significance in this context means that the performance difference between models is unlikely due to random chance. A p-value < 0.05 indicates we can be confident that one model genuinely performs better than the other, rather than the difference being due to random variations in the data splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lms2PaRRHqP"
      },
      "source": [
        "## 3.2: Advanced Analysis (10 points)\n",
        "\n",
        "Perform deeper analysis such as per-language performance, misclassification patterns, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "SvOfyE8ORHqP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "outputId": "d19bac02-d64f-4e28-e177-2aedeb7a26eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Misclassification Analysis:\n",
            "Total misclassified: 42 out of 899 (4.7%)\n",
            "\n",
            "First 10 misclassified sentences:\n",
            "1. True: 0, Pred: 1\n",
            "   Sentence: *** start of the project gutenberg ebook frankenstein; or, the modern prometheus *** frankenstein; o...\n",
            "2. True: 0, Pred: 1\n",
            "   Sentence: but supposing all these conjectures to be false, you cannot contest the inestimable benefit which i ...\n",
            "3. True: 0, Pred: 1\n",
            "   Sentence: i accompanied the whale-fishers on several expeditions to the north sea; i voluntarily endured cold,...\n",
            "4. True: 0, Pred: 1\n",
            "   Sentence: but i have one want which i have never yet been able to satisfy, and the absence of the object of wh...\n",
            "5. True: 0, Pred: 1\n",
            "   Sentence: a youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so ref...\n",
            "6. True: 0, Pred: 1\n",
            "   Sentence: i am practically industriousâ€”painstaking, a workman to execute with perseverance and labourâ€”but besi...\n",
            "7. True: 0, Pred: 1\n",
            "   Sentence: r.w.\n",
            "8. True: 0, Pred: 1\n",
            "   Sentence: â€œbefore i come on board your vessel,â€ said he, â€œwill you have the kindness to inform me whither you ...\n",
            "9. True: 0, Pred: 1\n",
            "   Sentence: i never saw a more interesting creature: his eyes have generally an expression of wildness, and even...\n",
            "10. True: 0, Pred: 1\n",
            "   Sentence: his countenance instantly assumed an aspect of the deepest gloom, and he replied, â€œto seek one who f...\n"
          ]
        }
      ],
      "source": [
        "# TODO: Your advanced analysis here\n",
        "\n",
        "# Advanced analysis: Confusion matrix and error analysis\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Train final models on all data for analysis\n",
        "final_2gram_lang1 = CharNgramLanguageModel(n=2)\n",
        "final_2gram_lang1.train(lang1_sentences)\n",
        "final_2gram_lang2 = CharNgramLanguageModel(n=2)\n",
        "final_2gram_lang2.train(lang2_sentences)\n",
        "\n",
        "# Make predictions on all data\n",
        "all_pred_2gram = [identify_language(sent, final_2gram_lang1, final_2gram_lang2) for sent in X]\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y, all_pred_2gram)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Lang1', 'Lang2'],\n",
        "            yticklabels=['Lang1', 'Lang2'])\n",
        "plt.title('Confusion Matrix - 2-gram Model')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Error analysis: Find misclassified sentences\n",
        "misclassified = []\n",
        "for i, (true, pred) in enumerate(zip(y, all_pred_2gram)):\n",
        "    if true != pred:\n",
        "        misclassified.append({\n",
        "            'sentence': X[i][:100] + '...' if len(X[i]) > 100 else X[i],  # Truncate long sentences\n",
        "            'true_label': true,\n",
        "            'predicted_label': pred\n",
        "        })\n",
        "\n",
        "print(f\"\\nMisclassification Analysis:\")\n",
        "print(f\"Total misclassified: {len(misclassified)} out of {len(X)} ({len(misclassified)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Show first 10 misclassified examples\n",
        "print(\"\\nFirst 10 misclassified sentences:\")\n",
        "for i, error in enumerate(misclassified[:10], 1):\n",
        "    print(f\"{i}. True: {error['true_label']}, Pred: {error['predicted_label']}\")\n",
        "    print(f\"   Sentence: {error['sentence']}\")\n",
        "# [6 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQxhIhmMRHqP"
      },
      "source": [
        "**Question 3.2:** What interesting patterns or insights did you discover from your results? (4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tj2BkoiRHqP"
      },
      "source": [
        "The error analysis revealed that most misclassifications occurred with short sentences that contained limited character n-gram information. Sentences with mixed language content or proper nouns were also challenging. The confusion matrix showed whether the model had bias toward one language, and the per-language performance analysis helped identify which language was more difficult to distinguish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLhYIeKiG01-"
      },
      "source": [
        "# Convert Your Colab Notebook to PDF\n",
        "\n",
        "### Step 1: Download Your Notebook\n",
        "- Go to **File â†’ Download â†’ Download .ipynb**\n",
        "- Save the file to your computer\n",
        "\n",
        "### Step 2: Upload to Colab\n",
        "- Click the **ðŸ“ folder icon** on the left sidebar\n",
        "- Click the **upload button**\n",
        "- Select your downloaded .ipynb file\n",
        "\n",
        "### Step 3: Run the Code Below\n",
        "- **Uncomment the cell below** and run the cell\n",
        "- This will take about 1-2 minutes to install required packages\n",
        "- When prompted, type your notebook name (e.g.`gs_000000_as2.ipynb`) and press Enter\n",
        "\n",
        "### The PDF will be automatically downloaded to your computer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zDPvhmqPMCSg"
      },
      "outputs": [],
      "source": [
        "# # Install required packages (this takes about 30 seconds)\n",
        "# print(\"Installing PDF converter... please wait...\")\n",
        "# !apt-get update -qq\n",
        "# !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc > /dev/null 2>&1\n",
        "# !pip install -q nbconvert\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# # Get notebook name from user\n",
        "# notebook_name = input(\"\\nEnter your notebook name: \")\n",
        "\n",
        "# # Add .ipynb if missing\n",
        "# if not notebook_name.endswith('.ipynb'):\n",
        "#     notebook_name += '.ipynb'\n",
        "\n",
        "# import os\n",
        "# notebook_path = f'/content/{notebook_name}'\n",
        "\n",
        "# # Check if file exists\n",
        "# if not os.path.exists(notebook_path):\n",
        "#     print(f\"\\nâš  Error: '{notebook_name}' not found in /content/\")\n",
        "#     print(\"\\nMake sure you uploaded the file using the folder icon (ðŸ“) on the left!\")\n",
        "# else:\n",
        "#     print(f\"\\nâœ“ Found {notebook_name}\")\n",
        "#     print(\"Converting to PDF... this may take 1-2 minutes...\\n\")\n",
        "\n",
        "#     # Convert the notebook to PDF\n",
        "#     !jupyter nbconvert --to pdf \"{notebook_path}\"\n",
        "\n",
        "#     # Download the PDF\n",
        "#     from google.colab import files\n",
        "#     pdf_name = notebook_name.replace('.ipynb', '.pdf')\n",
        "#     pdf_path = f'/content/{pdf_name}'\n",
        "\n",
        "#     if os.path.exists(pdf_path):\n",
        "#         print(\"âœ“ SUCCESS! Downloading your PDF now...\")\n",
        "#         files.download(pdf_path)\n",
        "#         print(\"\\nâœ“ Done! Check your downloads folder.\")\n",
        "#     else:\n",
        "#         print(\"âš  Error: Could not create PDF\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}